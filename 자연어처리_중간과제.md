**1.**
  강형석, 양장훈, 한국어 단어 임베딩을 위한 Word2vec 모델의 최적화, 2019. 디지털콘텐츠학회 논문지, Vol. 20, No. 4, pp.825-833

**2.**
  https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002462856

**3.**

**(1) 모델을 적용해서 해결하고자 한 문제**

**- 논문에서 해결하고자 하는 문제에 대한 설명**

  word2vec 모델을 한국어에 적용하려고 할 때, 학습 전 최적의 하이퍼파라미터를 세팅값이 무엇인지를 다루고 있다. word2vec 모델은 크게 4가지의 하이퍼파리미터를 세팅해야 하는데, 학습 알고리즘(CBOW or SG 중 택 1), 단어 벡터 크기, 문맥 윈도 크기, 최소 출현빈도가 그것이다. 기존 연구에 따르면 학습 알고리즘의 차이는 크지 않으며, 단어 벡터 크기는 300, 문맥 윈도 크기는 5~7, 최소 출현빈도는 말뭉치 corpus의 값에 따라 '절절히' 설정할 것만 요구했으며, 한국어의 중요한 특성이라고 할 수 있는 조사, 어미 등의 문법 형태소를 학습에서 제외시켰다.

  문법 형태소의 제외는 word2vec 모델을 감성 분류(sentiment classification)에 사용할 경우 문제가 될 수 있다. 따라서 문법 형태소를 포함하는 한국어 word2vec 모델의 최적 파라미터는 무엇인지 실험해 볼 필요가 있다.

    **분산 가설(distributional hypothesis)**
    비슷한 의미를 갖는 단어는 비슷한 문맥에서 등장한다

    분산 가설의 학습 알고리즘
      - CBOW(Countinuous Bag-Of-Words)
        문맥 윈도우(2k개의 주변 단어)가 주어졌을 때, 그 중심에 특정 단어가 나타날 조건부 확률
      - SG(Skip-Gram)
        중심 단어가 주어졌ㅇ르 때, 특정 조합의 주변 단어 2k개가 나타날 조건부 확률

      - Word2vec 모델은 말뭉치(corpus)의 문장을 스캔해 가면서, 조건부 확률이 최대가 되는 단어 벡터(word vector)의 값을 찾아낸다

    **Word2vec 모델의 성능 검증 방식**

      1) 유사도 검사
        영어 단어 임베딩 모델의 유사도 검사 : WordSim353 데이터 세트 (353개의 영단어 쌍과 해당 쌍의 유사도/관련도를 소수(13명 또는 16명)의 영어 화자가 평가한 점수(1~10)의 평균으로 구성한다. 만들어진 단어 벡터에서 두 단어 사이의 코사인 유사도를 측정하고, 검증 데이터 세트의 유사도 점수와 분포를 상관분석해서 pearson 계술르 구한 뒤, 이 값이 클수록 해당 단어 임베딩 모델의 성능이 높은 것으로 간주한다

        영어 모델이라는 한계. 353개라는 작은 데이터 세트, 13-16명이라는 충분하지 못한 화자

      2) 유추 검사
        GATS(Google Analogy Test Set) : 15개의 섹션(예 - 수도와 국가)으로 나누어진 총 19,544개의 문항(예 : "Athens-Greece-Cairo_Egypt)으로 구성되어 있다. 학습이 완료된 word2vec 모델에서 단어 Athens-Greece-Cairo_Egypt의 벡터를  v(Athens), v(Greece), v(Cairo), v(Egypt)로 하고, 학습된 전체 단어 집합을 W라고 한다면, Athens-Greece-Cairo_Egypt에 대한 유추 검사는 다음과 같은 벡터 관계식이 성립한다.

        $$v(Egypt) = argmin_{x -> W}|v(Athens) - v(Cairo) - (v(Greece) - x)|$$

        이때 x가 v(Egypt)와 같으면 해당 문항은 정답이 되고, 그 외에는 오답으로 처리한다

  **한국어 Word2vec 모델의 최적화**

        학습 알고리즘 외 하이퍼파라미터
          - 단어 벡터의 크기

        기존 연구에서의 최적 하이퍼파리미터(경험적 연구)
          - 단어 벡터 크기 = 300
          - 문맥 윈도 크기 : 5~7
          - 최소 출현빈도 : 말뭉치 크기에 따라 적절히 큰 값
          - 학습 알고리즘 : 차이 없음
        
**- 선택한 모델의 문제를 해결하는데 적합했던 이유에 대한 설명**

**(2) 논문에서 사용한 데이터에 관해 서술하시오**
**- 데이터의 출처나 수집 과정에 대한 설명** / **데이터의 크기, 형식 등의 특징을 포함한 데이터에 대한 설명** / 
**-데이터 전처리 과정에 대한 설명**

  이 논문에서 한국어 word2vec 모델링을 위해 사용한 학습 데이터세트는 나무위키와 위키피디아에서 덤프 파일을 내려받아, 문장부호, 수식, 외국어, 특수문자, url 등 한국어와 무관한 문자열들을 제거하였으며, 총 용량은 3.52GB이다. 
  토큰화를 위한 형태소 분석기 : Mecab 품사 태거(POS tagger) : 오픈 소스 형태로 공개되어 한국어 형태소 분석기 중 성능이 우수하다
  말뭉치는 6.6억개의 형태소(토큰)으로 구성되어 있고, 총 어휘 수는 형태소 기준으로 935,548개이다

**-데이터의 특성과 문제 해결과의 관련성에 대한 설명**






 

  
